{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "filename_data = open('mnist_data.pkl','rb')\n",
    "data = pickle.load(filename_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np.array(data['trainLabels'])\n",
    "testLabels = np.array(data['testLabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = np.array(data['testImages']).T\n",
    "trainData =np.array(data['trainImages']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : number or numpy array of numbers\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns sigmoid function value of a given number or array of numbers\n",
    "    '''\n",
    "    return 1/(1 + np.exp(-np.array(x)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : number or numpy array of numbers\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the derivative of Sigmoid at point x   \n",
    "    '''\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : numpy array of numbers\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns array of probabilities obtained by applying softmax function \n",
    "    '''\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "def encode_to_onehot(x):\n",
    "    '''This function encodes an integer label to one hot vector\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : list of integers (Range : 0-9)\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the one hot encoded vector of length 10 corresponding to each integer in the given array\n",
    "    '''\n",
    "    endoded_array = np.zeros((len(x), 10))\n",
    "    endoded_array[np.arange(len(endoded_array)), x] = 1\n",
    "    return endoded_array.T\n",
    "\n",
    "def onehot_to_num(x):\n",
    "    ''' This function converts onehot vector to corresponding integer label\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : Array of one-hot encoded vectors\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the integer value corresponding to each vector in the given array\n",
    "    '''\n",
    "    return np.argmax(x,0)\n",
    "    \n",
    "\n",
    "def initialize_parameters(no_of_hidden_neurons):\n",
    "    ''' This function initializes parameters for the MLP\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    no_of_hidden_neurons : integer denoting total no of neurons in the middle or hidden layer of 3 layer MLP\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns W1, B1, W2, B2\n",
    "            W1 is weight matrix and B1 is bias vector connecting input layer to hidden layer\n",
    "            W2 is weight matrix and B2 is bias vector connecting hidden layer to output layer\n",
    "    '''\n",
    "    W1 = np.random.rand(no_of_hidden_neurons,784) - 0.5\n",
    "    B1 = np.zeros((no_of_hidden_neurons,1))\n",
    "    W2 = np.random.rand(10,no_of_hidden_neurons) - 0.5\n",
    "    B2 = np.zeros((10,1))\n",
    "    return W1, B1, W2, B2\n",
    "\n",
    "def forward(x, W1, B1, W2, B2):\n",
    "    ''' This function is used for forward propogation of MLP\n",
    "    \n",
    "    Paramaters\n",
    "    ------------------\n",
    "    x : vector of length 784 representing the 28*28 image\n",
    "    W1, B1 : weight matrix and bias vector connecting input layer to hidden layer\n",
    "    W2, B2 : weight matrix and bias vector connecting hidden layer to output layer\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns Y1, layer1_out, Y2, layer2_out \n",
    "            Y1 : values of the hidden layer neurons before applying activation function\n",
    "            layer1_out : values of hidden layer after applying activation function\n",
    "            Y2 : values of the output layer neurons before applying activation function\n",
    "            layer2_out : values of output layer after applying activation function\n",
    "    \n",
    "    '''\n",
    "    Y1 = W1.dot(x) + B1\n",
    "    layer1_out = sigmoid(Y1)\n",
    "    Y2 = W2.dot(layer1_out) + B2\n",
    "    layer2_out = softmax(Y2)\n",
    "    return Y1, layer1_out, Y2, layer2_out\n",
    "\n",
    "def backPropogate(data, labels, learning_rate, W1, B1, W2, B2, Y1, layer1_out, Y2, layer2_out ):\n",
    "    ''' This function is used to backpropogate the error and update weights\n",
    "    \n",
    "    Paramaters\n",
    "    ------------------\n",
    "    data : 784 * n matrix representing n images\n",
    "    labels : vector of length n representing the target labels\n",
    "    learning_rate : learning rate to be used to update weights\n",
    "    W1, B1 : weight matrix and bias vector connecting input layer to hidden layer\n",
    "    W2, B2 : weight matrix and bias vector connecting hidden layer to output layer\n",
    "    Y1 : values of the hidden layer neurons before applying activation function\n",
    "    layer1_out : values of hidden layer after applying activation function\n",
    "    Y2 : values of the output layer neurons before applying activation function\n",
    "    layer2_out : values of output layer after applying activation function\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the updated W1, B1, W2, B2\n",
    "    '''\n",
    "    l = len(data[0])\n",
    "    labels_onehot = encode_to_onehot(labels)\n",
    "    der_Y2 = layer2_out - labels_onehot\n",
    "    der_W2 = 1/l * der_Y2.dot(layer1_out.T)\n",
    "    der_B2 = 1/l * np.sum(der_Y2)\n",
    "    der_Y1 = W2.T.dot(der_Y2) * sigmoid_derivative(Y1)\n",
    "    der_W1 = 1/l * der_Y1.dot(data.T)\n",
    "    der_B1 = 1/l * np.sum(der_Y1)\n",
    "    \n",
    "    W1 = W1 - learning_rate * der_W1\n",
    "    B1 = B1 - learning_rate * der_B1\n",
    "    W2 = W2 - learning_rate * der_W2\n",
    "    B2 = B2 - learning_rate * der_B2\n",
    "    \n",
    "    return W1, B1, W2, B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted, original):\n",
    "    ''' This function calculates the accuracy of predicted values\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    predicted : Array of integers representing predicted classes by MLP\n",
    "    original : Array of integers corresponding to true class\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns accuracy value\n",
    "    '''\n",
    "    return np.sum(predicted==original)/len(predicted)\n",
    "\n",
    "def calculate_loss(onehot_predicted, onehot_actual):\n",
    "    ''' This function calculates the  total loss of MLP network for given dataset\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    onehot_predicted : Array of onehot vectors predicted by MLP\n",
    "    onehot_actual : Array of true onehot vectors\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the loss value\n",
    "    '''\n",
    "    logs_matrix = np.log10(onehot_predicted.T+0.001)\n",
    "    return np.trace(-logs_matrix.dot(onehot_actual))\n",
    "    \n",
    "\n",
    "def train(X, Y, learning_rate, no_of_steps, no_of_hidden_neurons):\n",
    "    ''' This function trains the MLP network\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    X : 784 * n matrix representing tataset with n images\n",
    "    Y : vector of size n representing true labels\n",
    "    learning_rate : learning rate to be used to update weights\n",
    "    no_of_steps : Total steps to perform in the iteration\n",
    "    no_of_hidden_neurons : Total number of neurons to have in middle layer\n",
    "    \n",
    "    Output\n",
    "    -------------------\n",
    "    Returns the final W1, B1, W2, B2 representing the trained network\n",
    "    '''\n",
    "    W1, B1, W2, B2 = initialize_parameters(no_of_hidden_neurons)\n",
    "    Iterations_array = []\n",
    "    Accuracy_array = []\n",
    "    Loss_array = []\n",
    "    accuracy =0\n",
    "    loss = 0\n",
    "    for i in range(1,no_of_steps+1):\n",
    "        Y1, layer1_out, Y2, layer2_out = forward(X, W1, B1, W2, B2)\n",
    "        W1, B1, W2, B2 = backPropogate(X, Y, learning_rate, W1, B1, W2, B2, Y1, layer1_out, Y2, layer2_out )\n",
    "        if(i%10==0):            \n",
    "            predictions = onehot_to_num(layer2_out)\n",
    "            accuracy = calculate_accuracy(predictions, Y)\n",
    "            loss = calculate_loss(layer2_out, encode_to_onehot(Y))\n",
    "            print('Iteration: '+str(i)+' | Accuracy: '+str(accuracy) + ' | Loss: '+str(loss))\n",
    "            Iterations_array.append(i)\n",
    "            Accuracy_array.append(accuracy)\n",
    "            Loss_array.append(loss)\n",
    "    print('Training Accuracy: '+str(accuracy)+ ' | Training Loss: '+str(loss))\n",
    "    plt.plot(Iterations_array, Accuracy_array)\n",
    "    plt.title('Accuracy vs Iterations for Training Data')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('Accuracy_plot')\n",
    "    plt.show()\n",
    "    plt.plot(Iterations_array, Loss_array)\n",
    "    plt.title('Loss vs Iterations for Training Data')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('Loss_plot')\n",
    "    plt.show()\n",
    "    return W1, B1, W2, B2\n",
    "\n",
    "def test(test_X, test_y, W1, B1, W2, B2):\n",
    "    ''' This function prints the Test Accuracy and Loss\n",
    "    Paramaters\n",
    "    ------------------\n",
    "    test_X : 784 * n matrix representing tataset with n images\n",
    "    test_Y : vector of size n representing true labels\n",
    "    W1, B1, W2, B2 : The trained weights and biases  \n",
    "\n",
    "    '''\n",
    "    Y1, layer1_out, Y2, layer2_out = forward(test_X, W1, B1, W2, B2)\n",
    "    predictions = onehot_to_num(layer2_out)\n",
    "    accuracy = calculate_accuracy(predictions, test_y)\n",
    "    loss = calculate_loss(layer2_out, encode_to_onehot(test_y))\n",
    "    print('Test Accuracy: '+str(accuracy)+ ' | Test Loss: '+str(loss))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, B1, W2, B2 = train(trainData, trainLabels, 1, 250, 200)\n",
    "test(testData, testLabels, W1, B1, W2, B2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
